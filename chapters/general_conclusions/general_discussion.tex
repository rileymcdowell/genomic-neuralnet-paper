\section{General Discussion}

There are four primary goals of the work presented in this thesis.

\begin{itemize}
    \item Demonstrate whether neural network models can exhibit competitive 
          accuracy on public datasets.
    \item Explicitly determine the relationship between model regularization and 
          the genetic architecture of traits. 
    \item Determine the effect of adding additional hidden layers to neural 
          network models for genomic prediction.
    \item Explore the effect of GPU computing on neural network training time.
\end{itemize}

To achieve these goals, six public genomic datasets were collected and an
experimental framework was created to test several genomic prediction models.
A combination of CPU and GPU resources was used to fit nine statistical 
models including four neural network models to each of the six datasets 
while collecting information on training time as well as final 
prediction accuracy. 

Contrary to expectation, there was no consistent relationship between
the top performing models and the species or trait evaluated, however
several models performed notably worse than the others on all prediction
tasks (Table \ref{tab:model-comparison}). The unregularized ordinary
least squares and the unregularized neural network, as well as the L1
regularized LASSO were never top performers. The Bayesian ridge
regression model also was not a top performer, but performed notably
worse when predicting highly heritable traits. These results support the
hypothesis that regularization is required to achieve optimal performance
in neural network models and that the optimal model and regularization
strategy is dependent on the specific species, trait, or population
studied. Empirical validation of model accuracy is an effective way to 
select a model and regularization method for genomic prediction.

Neural networks were top performers on three of the six datasets
presented in this study. While this is a small sample size, it is
sufficient to demonstrate that neural networks can function as 
competitive genomic prediction models. Published studies to date have failed 
to apply regularization to networks, resulting in an underestimation 
of their predictive ability. A large body of work outside the plant 
and animal breeding literature demonstrates that networks can 
rapidly solve complex problems when applied appropriately.
Our results provide preliminary evidence that networks can be successfully
applied to plant breeding projects, comparison of regularized networks 
to RKHS and Bayesian Alphabet methods will be required before drawing conclusions about 
prediction accuracy.

The regularization methods presented in this paper included standard methods such 
as L1 and L2, as well as Bayesian prior distributions and network dropout. 
Given the relationship between regularization, genomic architecture, and model 
performance, it is not surprising that the two unregularized prediction methods, 
ordinary least squares and a standard MLP neural network, were not top performers 
on any dataset (Table \ref{tab:model-comparison}). The performance of the neural 
network models suggests that regularization is critical to achieve optimal 
performance. This is likely to be true for any problem which is predictive rather 
than descriptive in nature.

Adding additional hidden layers to the neural network models did not improve
network performance on the six genomic prediction tasks presented in this study.
This result was unexpected, as similar architectures have resulted in an outstanding 
gain in accuracy on both new and old prediction problems \citep{mnih2013, subasi2005, lang1990}.
There are several possible explanations for these results. First, the universal
approximation theorem predicts that a sufficiently large single layer network
can approximate any function, including those that multi-layer networks can approximate.
However, improved predictive accuracy from deep networks on some categories of problems
in other domains suggests that there are occasions where, 
at a minimum, deep networks train more effectively or reach training states that 
produce better predictive accuracy. It is not clear whether our results imply that
genomic prediction is best performed with neural networks with a single hidden 
layer or whether other neural network types might benefit from additional hidden layers.
A second possibility is that the dimensionality of the genomic prediction problems
presented in the selected datasets was insufficient to take advantage of the
flexible learning process of deep neural networks. Problems where deep networks have
made the largest improvements generally involve data that includes an additional dimension
such as one-dimensional time series or two-dimensional imagery data. 

Aside from linkage, the markers used as input for genomic prediction 
tasks are mostly independent. It is possible that additional data density 
such as full sequencing data which has an additional dimensional 
structure encoded in the order of nucleotides along a strand of 
DNA, is required to observe the effects of neural networks. A third possibility is that 
the measurement error on the phenotypes examined in this study was sufficiently 
large to mask the subtle interaction effects between markers that a deep 
network could potentially learn. If so, this problem could be resolved by reducing 
measurement error or increasing sampling sizes in genomic prediction training datasets.  

Previous studies have raised concerns about the computational complexity of fitting
neural network models \citep{heslot2012,gonzalez-recio2014}. Our results show
that training neural networks using GPU resources drastically
reduces the rate at which training time grows with sample size, marker count,
and network complexity (Figure \ref{fig:time-comparison}). The improvement in 
training time we observed was so large that growth in total time to train was sub-linear
across all three of these measures. This is because the compute capacity 
of modern GPUs is so large that the processing bottleneck is not the matrix algebra 
required perform backpropagation but rather moving data to and from the memory on 
the GPU hardware. Hardware manufacturers are aware of this limitation 
and continue to increase the speed of the hardware 
buses that move data between computer hardware components. Our results
demonstrate that concerns about computation complexity when fitting 
networks are no longer valid on small to medium sized datasets such as 
those presented in this paper. We anticipate that as hardware improves, 
even the largest of datasets will be able to move rapidly between hardware components, 
and the model fitting bottleneck will be capacity of the
GPU to perform algebraic manipulations, much like CPU hardware is for most 
model fitting tasks today.

